{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hdf5storage\n",
    "import librosa\n",
    "\n",
    "from model import *\n",
    "from tb_utils import *\n",
    "from preprocessing import *\n",
    "\n",
    "from scipy import io\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "default_envelope_config = {\n",
    "    'homomorphic_envelogram_with_hilbert': {'lpf_frequency': 8},\n",
    "    'psd': {'fl_low': 40, 'fl_high': 60, 'resample': True},\n",
    "    'wavelet': {'wavelet': 'db1',\n",
    "                'levels': [4],\n",
    "                'start_level': 1,\n",
    "                'end_level': 5,\n",
    "                'erase_pad': True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_groundtruth(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    data = data.split('\\n')\n",
    "\n",
    "    input_data = np.empty((len(data)-1, 3), dtype=float)  # Creazione di un array vuoto con le dimensioni corrette\n",
    "\n",
    "    for i in range(len(data)-1):\n",
    "        row_values = data[i].split('\\t')\n",
    "        for j in range(len(row_values)):\n",
    "            input_data[i, j] = float(row_values[j])\n",
    "\n",
    "    # Find the duration in seconds of the recording (last value of the second column)\n",
    "    T = input_data[-1,1]\n",
    "\n",
    "    # Generate a time array with 50 Hz sampling rate\n",
    "    t = np.arange(0, T, 1.0/50.0)\n",
    "\n",
    "    # Generate the s array\n",
    "    s = np.zeros_like(t)\n",
    "\n",
    "    # For each time instant, find its heart state and assign it to the s array\n",
    "    for i in range(len(t)):\n",
    "        # Find the first value greater than t(i)\n",
    "        j = np.argmax(input_data[:,1] > t[i])\n",
    "        # Assign the heart state to s(i)\n",
    "        s[i] = input_data[j,2]\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(x_enve,s_enve,N,tau):\n",
    "    X = np.zeros((0, 4, N))\n",
    "    S = np.zeros((0, N))\n",
    "\n",
    "    # Get envelopes\n",
    "    x_global = x_enve\n",
    "    s_global = s_enve\n",
    "\n",
    "\n",
    "    # Extract the indexes of the s elements with heart state information\n",
    "    labeled_idxs_global = np.where(s_global!=0)[0]\n",
    "\n",
    "    # Find 0 intervals between heart state changes\n",
    "    zero_intervals = np.diff(labeled_idxs_global)-1 != 0\n",
    "\n",
    "    # Split x and s between those intervals\n",
    "    x_split = np.split(x_global, labeled_idxs_global[1:][zero_intervals], axis=1)\n",
    "    s_split = np.split(s_global, labeled_idxs_global[1:][zero_intervals])\n",
    "\n",
    "    for k in range(len(x_split)):\n",
    "\n",
    "        # Extract the indexes of the s elements with heart state information\n",
    "        labeled_idxs = np.where(s_split[k]!=0)[0]\n",
    "\n",
    "        # Use only data with heart state information\n",
    "        x = x_split[k][:, labeled_idxs]\n",
    "        s = s_split[k][labeled_idxs]\n",
    "\n",
    "        if x.shape[1] < N:\n",
    "            # If the window is smaller than N, discard it\n",
    "            continue\n",
    "\n",
    "        x = rolling_strided_window(x, N, tau)\n",
    "        s = rolling_strided_window(s, N, tau)\n",
    "\n",
    "        x, s = check_valid_sequence(x, s, 1)\n",
    "\n",
    "        # Stack the windows\n",
    "        X = np.vstack((X, x))\n",
    "        S = np.vstack((S, s))\n",
    "        \n",
    "    # Create a new axis in S and concatenate X and S\n",
    "    S = S[:, np.newaxis, :]\n",
    "    XS = np.concatenate((X, S), axis=1)\n",
    "\n",
    "    # Shuffle the samples\n",
    "    #np.random.shuffle(XS)\n",
    "\n",
    "    # Return the X and S arrays\n",
    "    X = XS[:, :x.shape[1], :]\n",
    "    S = XS[:, x.shape[1]:, :]\n",
    "\n",
    "    # Swap axes to format the data as channels_last\n",
    "    X = np.swapaxes(X, 1, 2)\n",
    "    S = np.swapaxes(S, 1, 2)\n",
    "\n",
    "    # Transform S to categorical\n",
    "    S = to_categorical(S-1)\n",
    "\n",
    "    return X,S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_strided_windows(S: np.ndarray, tau: int) -> np.ndarray:\n",
    "    \"\"\"Unrolls the input `S` 3D array with shape (n_windows, N, 4), which is\n",
    "    supposed to be generated with stride `tau`, outputing a 2D vector. The\n",
    "    elements in the overlapping positions are averaged.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        S (np.ndarray): Input 3D array.\n",
    "        tau (int): Stride of the input array.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        s (np.ndarray): 2D array with shape (tau*(n_windows-1) + N, 4).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check that the input array is a np.ndarray\n",
    "    if not isinstance(S, np.ndarray):\n",
    "        raise TypeError('Input array must be a np.ndarray')\n",
    "    \n",
    "    # Check that the input array is 2D\n",
    "    if S.ndim != 3:\n",
    "        raise ValueError('Input array must be 3D.')\n",
    "\n",
    "    # Check that the stride is positive integer\n",
    "    if not isinstance(tau, int):\n",
    "        raise TypeError('Stride must be an integer.')\n",
    "\n",
    "    # Obtain the window size and the number of windows\n",
    "    N = S.shape[1]\n",
    "    n_windows = S.shape[0]\n",
    "\n",
    "    # Calculate the length of the output array\n",
    "    s_len = tau*(n_windows-1) + N\n",
    "\n",
    "    # Create a 2D array of NaNs of size (n_windows, s_len)\n",
    "    s_expanded = np.full((n_windows, s_len, 4), np.nan)\n",
    "\n",
    "    # Allocate each window to the corresponding position in the expanded array\n",
    "    for i in range(n_windows):\n",
    "        s_expanded[i, tau*i:tau*i+N, :] = S[i, :, :]\n",
    "    \n",
    "    # Calculate the mean of the expanded array in the first axis\n",
    "    s = np.nanmean(s_expanded, axis=0)\n",
    "\n",
    "    return np.squeeze(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_max_temporal_model(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Implementation of the sequential max temporal modeling. It forces the input\n",
    "    states sequence to contain only admisible transitions among heart states\n",
    "    (S1->systolic->S2->diastolic->S1).\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        x (np.ndarray): Input sequence of states. The elements must be integers\n",
    "        between 1 and 4.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        y (np.ndarray): Output sequence of states, where only admisible\n",
    "        transitions are present.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if x is a numpy array of 1D\n",
    "    if not isinstance(x, np.ndarray) or x.ndim != 1:\n",
    "        raise TypeError('x must be a numpy array of 1D.')\n",
    "    \n",
    "    # Create y as an empty array of same size as x\n",
    "    y = np.zeros(x.shape)\n",
    "\n",
    "    # Set that the first element of y is the same as the first element of x\n",
    "    y[0] = x[0]\n",
    "\n",
    "    # Iterate over the rest of the elements of x\n",
    "    for i in range(1, x.shape[0]):\n",
    "        # If x[i] = (x[i-1] + 1) % 4, then y[i] = x[i]\n",
    "        if y[i-1] % 4 + 1 == x[i]:\n",
    "            y[i] = x[i]\n",
    "        # Otherwise, y[i] = y[i-1]\n",
    "        else:\n",
    "            y[i] = y[i-1]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load recordings\n",
    "recording_2530, frequency = librosa.load('./2530_AV.wav', sr=None)\n",
    "recording_14241, frequency = librosa.load('./14241_PV.wav', sr=None)\n",
    "recording_23625, frequency = librosa.load('./23625_MV.wav', sr=None)\n",
    "recording_24160, frequency = librosa.load('./24160_MV.wav', sr=None)\n",
    "recording_40840, frequency = librosa.load('./40840_TV.wav', sr=None)\n",
    "\n",
    "#Renna pre-processing\n",
    "pre_proc_data_2530_orig=renna_preprocess_wave(input_signal=recording_2530, fs=4000, config_dict=default_envelope_config) #WITH final standardization\n",
    "pre_proc_data_14241_orig=renna_preprocess_wave(input_signal=recording_14241, fs=4000, config_dict=default_envelope_config)\n",
    "pre_proc_data_23625_orig=renna_preprocess_wave(input_signal=recording_23625, fs=4000, config_dict=default_envelope_config)\n",
    "pre_proc_data_24160_orig=renna_preprocess_wave(input_signal=recording_24160, fs=4000, config_dict=default_envelope_config)\n",
    "pre_proc_data_40840_orig=renna_preprocess_wave(input_signal=recording_40840, fs=4000, config_dict=default_envelope_config)\n",
    "\n",
    "#Load CNN\n",
    "model = get_model()\n",
    "model.load_weights('parameters.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2530 Q32\n",
    "envelopes_2530Q32 = [] #appending order is the same as the one in \"preprocessing.py\"\n",
    "envelopes_2530Q32.append(np.squeeze(hdf5storage.loadmat('./2530_data/homo_test_2530.mat')['homo_test'])[0:pre_proc_data_2530_orig.shape[1]])\n",
    "envelopes_2530Q32.append(np.squeeze(hdf5storage.loadmat('./2530_data/hilb_test_2530.mat')['hilb_test'])[0+1:pre_proc_data_2530_orig.shape[1]+1])\n",
    "envelopes_2530Q32.append(np.squeeze(hdf5storage.loadmat('./2530_data/psd_test_2530.mat')['psd_test'])[0+1:pre_proc_data_2530_orig.shape[1]+1]) #I have to correct the amount of group delay in the logic, thus I do it here temporaney, visually assessed\n",
    "envelopes_2530Q32.append(np.squeeze(hdf5storage.loadmat('./2530_data/swt_test_2530.mat')['swt_test'])[0:pre_proc_data_2530_orig.shape[1]])\n",
    "\n",
    "envelopes_2530Q32 = np.stack(envelopes_2530Q32, axis=0)\n",
    "envelopes_2530Q32 = (envelopes_2530Q32 - envelopes_2530Q32.mean(axis=1, keepdims=True)) / envelopes_2530Q32.std(axis=1, keepdims=True)\n",
    "\n",
    "#2530 Q16\n",
    "envelopes_2530Q16 = []\n",
    "envelopes_2530Q16.append(np.squeeze(hdf5storage.loadmat('./2530_data_Q16/homo_env_2530Q16.mat')['homo_env'])[0:pre_proc_data_2530_orig.shape[1]])\n",
    "envelopes_2530Q16.append(np.squeeze(hdf5storage.loadmat('./2530_data_Q16/hilb_env_2530Q16.mat')['hilb_env'])[0+1:pre_proc_data_2530_orig.shape[1]+1])\n",
    "envelopes_2530Q16.append(np.squeeze(hdf5storage.loadmat('./2530_data_Q16/psd_env_2530Q16.mat')['psd_env'])[0+1:pre_proc_data_2530_orig.shape[1]+1])\n",
    "envelopes_2530Q16.append(np.squeeze(hdf5storage.loadmat('./2530_data_Q16/swt_env_2530Q16.mat')['swt_env'])[0:pre_proc_data_2530_orig.shape[1]])\n",
    "\n",
    "envelopes_2530Q16 = np.stack(envelopes_2530Q16, axis=0)\n",
    "envelopes_2530Q16 = (envelopes_2530Q16 - envelopes_2530Q16.mean(axis=1, keepdims=True)) / envelopes_2530Q16.std(axis=1, keepdims=True)\n",
    "\n",
    "#14241 Q32\n",
    "envelopes_14241Q32 = []\n",
    "envelopes_14241Q32.append(np.squeeze(hdf5storage.loadmat('./14241_data/homo_env_14241.mat')['homo_env'])[0:pre_proc_data_14241_orig.shape[1]])\n",
    "envelopes_14241Q32.append(np.squeeze(hdf5storage.loadmat('./14241_data/hilb_env_14241.mat')['hilb_env'])[0+1:pre_proc_data_14241_orig.shape[1]+1])\n",
    "envelopes_14241Q32.append(np.squeeze(hdf5storage.loadmat('./14241_data/psd_env_14241.mat')['psd_env'])[0+1:pre_proc_data_14241_orig.shape[1]+1])\n",
    "envelopes_14241Q32.append(np.squeeze(hdf5storage.loadmat('./14241_data/swt_env_14241.mat')['swt_env'])[0:pre_proc_data_14241_orig.shape[1]])\n",
    "\n",
    "envelopes_14241Q32 = np.stack(envelopes_14241Q32, axis=0)\n",
    "envelopes_14241Q32 = (envelopes_14241Q32 - envelopes_14241Q32.mean(axis=1, keepdims=True)) / envelopes_14241Q32.std(axis=1, keepdims=True)\n",
    "\n",
    "#14241 Q16\n",
    "envelopes_14241Q16 = []\n",
    "envelopes_14241Q16.append(np.squeeze(hdf5storage.loadmat('./14241_data_Q16/homo_env_14241Q16.mat')['homo_env'])[0:pre_proc_data_14241_orig.shape[1]])\n",
    "envelopes_14241Q16.append(np.squeeze(hdf5storage.loadmat('./14241_data_Q16/hilb_env_14241Q16.mat')['hilb_env'])[0+1:pre_proc_data_14241_orig.shape[1]+1])\n",
    "envelopes_14241Q16.append(np.squeeze(hdf5storage.loadmat('./14241_data_Q16/psd_env_14241Q16.mat')['psd_env'])[0+1:pre_proc_data_14241_orig.shape[1]+1])\n",
    "envelopes_14241Q16.append(np.squeeze(hdf5storage.loadmat('./14241_data_Q16/swt_env_14241Q16.mat')['swt_env'])[0:pre_proc_data_14241_orig.shape[1]])\n",
    "\n",
    "envelopes_14241Q16 = np.stack(envelopes_14241Q16, axis=0)\n",
    "envelopes_14241Q16 = (envelopes_14241Q16 - envelopes_14241Q16.mean(axis=1, keepdims=True)) / envelopes_14241Q16.std(axis=1, keepdims=True)\n",
    "\n",
    "#23625 Q32\n",
    "envelopes_23625Q32 = []\n",
    "envelopes_23625Q32.append(np.squeeze(hdf5storage.loadmat('./23625_data/homo_env_23625Q32.mat')['homo_env'])[0:pre_proc_data_23625_orig.shape[1]])\n",
    "envelopes_23625Q32.append(np.squeeze(hdf5storage.loadmat('./23625_data/hilb_env_23625Q32.mat')['hilb_env'])[0+1:pre_proc_data_23625_orig.shape[1]+1])\n",
    "envelopes_23625Q32.append(np.squeeze(hdf5storage.loadmat('./23625_data/psd_env_23625Q32.mat')['psd_env'])[0+1:pre_proc_data_23625_orig.shape[1]+1])\n",
    "envelopes_23625Q32.append(np.squeeze(hdf5storage.loadmat('./23625_data/swt_env_23625Q32.mat')['swt_env'])[0:pre_proc_data_23625_orig.shape[1]])\n",
    "\n",
    "envelopes_23625Q32 = np.stack(envelopes_23625Q32, axis=0)\n",
    "envelopes_23625Q32 = (envelopes_23625Q32 - envelopes_23625Q32.mean(axis=1, keepdims=True)) / envelopes_23625Q32.std(axis=1, keepdims=True)\n",
    "\n",
    "#23625 Q16\n",
    "envelopes_23625Q16 = []\n",
    "envelopes_23625Q16.append(np.squeeze(hdf5storage.loadmat('./23625_data_Q16/homo_env_23625Q16.mat')['homo_env'])[0:pre_proc_data_23625_orig.shape[1]])\n",
    "envelopes_23625Q16.append(np.squeeze(hdf5storage.loadmat('./23625_data_Q16/hilb_env_23625Q16.mat')['hilb_env'])[0+1:pre_proc_data_23625_orig.shape[1]+1])\n",
    "envelopes_23625Q16.append(np.squeeze(hdf5storage.loadmat('./23625_data_Q16/psd_env_23625Q16.mat')['psd_env'])[0+1:pre_proc_data_23625_orig.shape[1]+1])\n",
    "envelopes_23625Q16.append(np.squeeze(hdf5storage.loadmat('./23625_data_Q16/swt_env_23625Q16.mat')['swt_env'])[0:pre_proc_data_23625_orig.shape[1]])\n",
    "\n",
    "envelopes_23625Q16 = np.stack(envelopes_23625Q16, axis=0)\n",
    "envelopes_23625Q16 = (envelopes_23625Q16 - envelopes_23625Q16.mean(axis=1, keepdims=True)) / envelopes_23625Q16.std(axis=1, keepdims=True)\n",
    "\n",
    "#24160 Q32\n",
    "envelopes_24160Q32 = []\n",
    "envelopes_24160Q32.append(np.squeeze(hdf5storage.loadmat('./24160_data/homo_env_24160.mat')['homo_env'])[0:pre_proc_data_24160_orig.shape[1]])\n",
    "envelopes_24160Q32.append(np.squeeze(hdf5storage.loadmat('./24160_data/hilb_env_24160.mat')['hilb_env'])[0+1:pre_proc_data_24160_orig.shape[1]+1])\n",
    "envelopes_24160Q32.append(np.squeeze(hdf5storage.loadmat('./24160_data/psd_env_24160.mat')['psd_env'])[0+1:pre_proc_data_24160_orig.shape[1]+1])\n",
    "envelopes_24160Q32.append(np.squeeze(hdf5storage.loadmat('./24160_data/swt_env_24160.mat')['swt_env'])[0:pre_proc_data_24160_orig.shape[1]])\n",
    "\n",
    "envelopes_24160Q32 = np.stack(envelopes_24160Q32, axis=0)\n",
    "envelopes_24160Q32 = (envelopes_24160Q32 - envelopes_24160Q32.mean(axis=1, keepdims=True)) / envelopes_24160Q32.std(axis=1, keepdims=True)\n",
    "\n",
    "#24160 Q16\n",
    "envelopes_24160Q16 = []\n",
    "envelopes_24160Q16.append(np.squeeze(hdf5storage.loadmat('./24160_data_Q16/homo_env_24160Q16.mat')['homo_env'])[0:pre_proc_data_24160_orig.shape[1]])\n",
    "envelopes_24160Q16.append(np.squeeze(hdf5storage.loadmat('./24160_data_Q16/hilb_env_24160Q16.mat')['hilb_env'])[0+1:pre_proc_data_24160_orig.shape[1]+1])\n",
    "envelopes_24160Q16.append(np.squeeze(hdf5storage.loadmat('./24160_data_Q16/psd_env_24160Q16.mat')['psd_env'])[0+1:pre_proc_data_24160_orig.shape[1]+1])\n",
    "envelopes_24160Q16.append(np.squeeze(hdf5storage.loadmat('./24160_data_Q16/swt_env_24160Q16.mat')['swt_env'])[0:pre_proc_data_24160_orig.shape[1]])\n",
    "\n",
    "envelopes_24160Q16 = np.stack(envelopes_24160Q16, axis=0)\n",
    "envelopes_24160Q16 = (envelopes_24160Q16 - envelopes_24160Q16.mean(axis=1, keepdims=True)) / envelopes_24160Q16.std(axis=1, keepdims=True)\n",
    "\n",
    "#40840 Q32\n",
    "envelopes_40840Q32 = []\n",
    "envelopes_40840Q32.append(np.squeeze(hdf5storage.loadmat('./40840_data/homo_env_40840.mat')['homo_env'])[0:pre_proc_data_40840_orig.shape[1]])\n",
    "envelopes_40840Q32.append(np.squeeze(hdf5storage.loadmat('./40840_data/hilb_env_40840.mat')['hilb_env'])[0+1:pre_proc_data_40840_orig.shape[1]+1])\n",
    "envelopes_40840Q32.append(np.squeeze(hdf5storage.loadmat('./40840_data/psd_env_40840.mat')['psd_env'])[0+1:pre_proc_data_40840_orig.shape[1]+1])\n",
    "envelopes_40840Q32.append(np.squeeze(hdf5storage.loadmat('./40840_data/swt_env_40840.mat')['swt_env'])[0:pre_proc_data_40840_orig.shape[1]])\n",
    "\n",
    "envelopes_40840Q32 = np.stack(envelopes_40840Q32, axis=0)\n",
    "envelopes_40840Q32 = (envelopes_40840Q32 - envelopes_40840Q32.mean(axis=1, keepdims=True)) / envelopes_40840Q32.std(axis=1, keepdims=True)\n",
    "\n",
    "#40840 Q16\n",
    "envelopes_40840Q16 = []\n",
    "envelopes_40840Q16.append(np.squeeze(hdf5storage.loadmat('./40840_data_Q16/homo_env_40840Q16.mat')['homo_env'])[0:pre_proc_data_40840_orig.shape[1]])\n",
    "envelopes_40840Q16.append(np.squeeze(hdf5storage.loadmat('./40840_data_Q16/hilb_env_40840Q16.mat')['hilb_env'])[0+1:pre_proc_data_40840_orig.shape[1]+1])\n",
    "envelopes_40840Q16.append(np.squeeze(hdf5storage.loadmat('./40840_data_Q16/psd_env_40840Q16.mat')['psd_env'])[0+1:pre_proc_data_40840_orig.shape[1]+1])\n",
    "envelopes_40840Q16.append(np.squeeze(hdf5storage.loadmat('./40840_data_Q16/swt_env_40840Q16.mat')['swt_env'])[0:pre_proc_data_40840_orig.shape[1]])\n",
    "\n",
    "envelopes_40840Q16 = np.stack(envelopes_40840Q16, axis=0)\n",
    "envelopes_40840Q16 = (envelopes_40840Q16 - envelopes_40840Q16.mean(axis=1, keepdims=True)) / envelopes_40840Q16.std(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_2530 = load_groundtruth(\"2530_AV.tsv\")\n",
    "s_14241 = load_groundtruth(\"14241_PV.tsv\")\n",
    "s_23625 = load_groundtruth(\"23625_MV.tsv\") #it's a plus\n",
    "s_24160 = load_groundtruth(\"24160_MV.tsv\")\n",
    "s_40840 = load_groundtruth(\"40840_TV.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=64\n",
    "tau=8\n",
    "\n",
    "X_2530Q32, S_2530Q32 = prepare_dataset(envelopes_2530Q32,s_2530,N,tau) #function adapted from Daniel cose \"generate_X_S_2022()\" in utils.preprocessing.py see: https://github.com/eneriz-daniel/PCG-Segmentation-Model-Optimization/blob/master/training/utils/preprocessing.py\n",
    "X_2530Q16, S_2530Q16 = prepare_dataset(envelopes_2530Q16,s_2530,N,tau)\n",
    "X_2530orig, S_2530orig = prepare_dataset(pre_proc_data_2530_orig,s_2530,N,tau)\n",
    "\n",
    "X_14241Q32, S_14241Q32 = prepare_dataset(envelopes_14241Q32,s_14241,N,tau)\n",
    "X_14241Q16, S_14241Q16 = prepare_dataset(envelopes_14241Q16,s_14241,N,tau)\n",
    "X_14241orig, S_14241orig = prepare_dataset(pre_proc_data_14241_orig,s_14241,N,tau)\n",
    "\n",
    "X_23625Q32, S_23625Q32 = prepare_dataset(envelopes_23625Q32,s_23625,N,tau)\n",
    "X_23625Q16, S_23625Q16 = prepare_dataset(envelopes_23625Q16,s_23625,N,tau)\n",
    "X_23625orig, S_23625orig = prepare_dataset(pre_proc_data_23625_orig,s_23625,N,tau)\n",
    "\n",
    "X_24160Q32, S_24160Q32 = prepare_dataset(envelopes_24160Q32,s_24160,N,tau)\n",
    "X_24160Q16, S_24160Q16 = prepare_dataset(envelopes_24160Q16,s_24160,N,tau)\n",
    "X_24160orig, S_24160orig = prepare_dataset(pre_proc_data_24160_orig,s_24160,N,tau)\n",
    "\n",
    "X_40840Q32, S_40840Q32 = prepare_dataset(envelopes_40840Q32,s_40840,N,tau)\n",
    "X_40840Q16, S_40840Q16 = prepare_dataset(envelopes_40840Q16,s_40840,N,tau)\n",
    "X_40840orig, S_40840orig = prepare_dataset(pre_proc_data_40840_orig,s_40840,N,tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
     ]
    }
   ],
   "source": [
    "pred_2530Q32 = model.predict(X_2530Q32)\n",
    "pred_2530Q16 = model.predict(X_2530Q16)\n",
    "pred_2530_orig = model.predict(X_2530orig)\n",
    "\n",
    "pred_14241Q32 = model.predict(X_14241Q32)\n",
    "pred_14241Q16 = model.predict(X_14241Q16)\n",
    "pred_14241_orig = model.predict(X_14241orig)\n",
    "\n",
    "pred_23625Q32 = model.predict(X_23625Q32)\n",
    "pred_23625Q16 = model.predict(X_23625Q16)\n",
    "pred_23625_orig = model.predict(X_23625orig)\n",
    "\n",
    "pred_24160Q32 = model.predict(X_24160Q32)\n",
    "pred_24160Q16 = model.predict(X_24160Q16)\n",
    "pred_24160_orig = model.predict(X_24160orig)\n",
    "\n",
    "pred_40840Q32 = model.predict(X_40840Q32)\n",
    "pred_40840Q16 = model.predict(X_40840Q16)\n",
    "pred_40840_orig = model.predict(X_40840orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pred2530Q32 = np.squeeze((unroll_strided_windows(pred_2530Q32, tau)).argmax(axis=-1)+1)\n",
    "s_true2530Q32 = np.squeeze((unroll_strided_windows(S_2530Q32, tau)).argmax(axis=-1)+1)\n",
    "s_pred2530Q16 = np.squeeze((unroll_strided_windows(pred_2530Q16, tau)).argmax(axis=-1)+1)\n",
    "s_true2530Q16 = np.squeeze((unroll_strided_windows(S_2530Q16, tau)).argmax(axis=-1)+1)\n",
    "s_pred2530orig = np.squeeze((unroll_strided_windows(pred_2530_orig, tau)).argmax(axis=-1)+1)\n",
    "s_true2530orig = np.squeeze((unroll_strided_windows(S_2530orig, tau)).argmax(axis=-1)+1)\n",
    "\n",
    "s_pred14241Q32 = np.squeeze((unroll_strided_windows(pred_14241Q32, tau)).argmax(axis=-1)+1)\n",
    "s_true14241Q32 = np.squeeze((unroll_strided_windows(S_14241Q32, tau)).argmax(axis=-1)+1)\n",
    "s_pred14241Q16 = np.squeeze((unroll_strided_windows(pred_14241Q16, tau)).argmax(axis=-1)+1)\n",
    "s_true14241Q16 = np.squeeze((unroll_strided_windows(S_14241Q16, tau)).argmax(axis=-1)+1)\n",
    "s_pred14241orig = np.squeeze((unroll_strided_windows(pred_14241_orig, tau)).argmax(axis=-1)+1)\n",
    "s_true14241orig = np.squeeze((unroll_strided_windows(S_14241orig, tau)).argmax(axis=-1)+1)\n",
    "\n",
    "s_pred23625Q32 = np.squeeze((unroll_strided_windows(pred_23625Q32, tau)).argmax(axis=-1)+1)\n",
    "s_true23625Q32 = np.squeeze((unroll_strided_windows(S_23625Q32, tau)).argmax(axis=-1)+1)\n",
    "s_pred23625Q16 = np.squeeze((unroll_strided_windows(pred_23625Q16, tau)).argmax(axis=-1)+1)\n",
    "s_true23625Q16 = np.squeeze((unroll_strided_windows(S_23625Q16, tau)).argmax(axis=-1)+1)\n",
    "s_pred23625orig = np.squeeze((unroll_strided_windows(pred_23625_orig, tau)).argmax(axis=-1)+1)\n",
    "s_true23625orig = np.squeeze((unroll_strided_windows(S_23625orig, tau)).argmax(axis=-1)+1)\n",
    "\n",
    "s_pred24160Q32 = np.squeeze((unroll_strided_windows(pred_24160Q32, tau)).argmax(axis=-1)+1)\n",
    "s_true24160Q32 = np.squeeze((unroll_strided_windows(S_24160Q32, tau)).argmax(axis=-1)+1)\n",
    "s_pred24160Q16 = np.squeeze((unroll_strided_windows(pred_24160Q16, tau)).argmax(axis=-1)+1)\n",
    "s_true24160Q16 = np.squeeze((unroll_strided_windows(S_24160Q16, tau)).argmax(axis=-1)+1)\n",
    "s_pred24160orig = np.squeeze((unroll_strided_windows(pred_24160_orig, tau)).argmax(axis=-1)+1)\n",
    "s_true24160orig = np.squeeze((unroll_strided_windows(S_24160orig, tau)).argmax(axis=-1)+1)\n",
    "\n",
    "s_pred40840Q32 = np.squeeze((unroll_strided_windows(pred_40840Q32, tau)).argmax(axis=-1)+1)\n",
    "s_true40840Q32 = np.squeeze((unroll_strided_windows(S_40840Q32, tau)).argmax(axis=-1)+1)\n",
    "s_pred40840Q16 = np.squeeze((unroll_strided_windows(pred_40840Q16, tau)).argmax(axis=-1)+1)\n",
    "s_true40840Q16 = np.squeeze((unroll_strided_windows(S_40840Q16, tau)).argmax(axis=-1)+1)\n",
    "s_pred40840orig = np.squeeze((unroll_strided_windows(pred_40840_orig, tau)).argmax(axis=-1)+1)\n",
    "s_true40840orig = np.squeeze((unroll_strided_windows(S_40840orig, tau)).argmax(axis=-1)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pred2530Q32_stm = seq_max_temporal_model(s_pred2530Q32)\n",
    "s_pred2530Q16_stm = seq_max_temporal_model(s_pred2530Q16)\n",
    "s_pred2530orig_stm = seq_max_temporal_model(s_pred2530orig)\n",
    "\n",
    "s_pred14241Q32_stm = seq_max_temporal_model(s_pred14241Q32)\n",
    "s_pred14241Q16_stm = seq_max_temporal_model(s_pred14241Q16)\n",
    "s_pred14241orig_stm = seq_max_temporal_model(s_pred14241orig)\n",
    "\n",
    "s_pred23625Q32_stm = seq_max_temporal_model(s_pred23625Q32)\n",
    "s_pred23625Q16_stm = seq_max_temporal_model(s_pred23625Q16)\n",
    "s_pred23625orig_stm = seq_max_temporal_model(s_pred23625orig)\n",
    "\n",
    "s_pred24160Q32_stm = seq_max_temporal_model(s_pred24160Q32)\n",
    "s_pred24160Q16_stm = seq_max_temporal_model(s_pred24160Q16)\n",
    "s_pred24160orig_stm = seq_max_temporal_model(s_pred24160orig)\n",
    "\n",
    "s_pred40840Q32_stm = seq_max_temporal_model(s_pred40840Q32)\n",
    "s_pred40840Q16_stm = seq_max_temporal_model(s_pred40840Q16)\n",
    "s_pred40840orig_stm = seq_max_temporal_model(s_pred40840orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2530 Orig ACC:  0.92\n",
      "2530 Q32 ACC:  0.90\n",
      "2530 Q16 ACC:  0.90\n",
      "\n",
      "14241 Orig ACC:  0.91\n",
      "14241 Q32 ACC:  0.90\n",
      "14241 Q16 ACC:  0.90\n",
      "\n",
      "23625 Orig ACC:  0.93\n",
      "23625 Q32 ACC:  0.91\n",
      "23625 Q16 ACC:  0.91\n",
      "\n",
      "24160 Orig ACC:  0.88\n",
      "24160 Q32 ACC:  0.68\n",
      "24160 Q16 ACC:  0.67\n",
      "\n",
      "40840 Orig ACC:  0.91\n",
      "40840 Q32 ACC:  0.91\n",
      "40840 Q16 ACC:  0.90\n"
     ]
    }
   ],
   "source": [
    "acc_2530Q32 = accuracy_score(s_true2530Q32, s_pred2530Q32_stm, normalize=True)\n",
    "acc_2530Q16 = accuracy_score(s_true2530Q16, s_pred2530Q16_stm, normalize=True)\n",
    "acc_2530orig = accuracy_score(s_true2530orig, s_pred2530orig_stm, normalize=True)\n",
    "\n",
    "acc_14241Q32 = accuracy_score(s_true14241Q32, s_pred14241Q32_stm, normalize=True)\n",
    "acc_14241Q16 = accuracy_score(s_true14241Q16, s_pred14241Q16_stm, normalize=True)\n",
    "acc_14241orig = accuracy_score(s_true14241orig, s_pred14241orig_stm, normalize=True)\n",
    "\n",
    "acc_23625Q32 = accuracy_score(s_true23625Q32, s_pred23625Q32_stm, normalize=True)\n",
    "acc_23625Q16 = accuracy_score(s_true23625Q16, s_pred23625Q16_stm, normalize=True)\n",
    "acc_23625orig = accuracy_score(s_true23625orig, s_pred23625orig_stm, normalize=True)\n",
    "\n",
    "acc_24160Q32 = accuracy_score(s_true24160Q32, s_pred24160Q32_stm, normalize=True)\n",
    "acc_24160Q16 = accuracy_score(s_true24160Q16, s_pred24160Q16_stm, normalize=True)\n",
    "acc_24160orig = accuracy_score(s_true24160orig, s_pred24160orig_stm, normalize=True)\n",
    "\n",
    "acc_40840Q32 = accuracy_score(s_true40840Q32, s_pred40840Q32_stm, normalize=True)\n",
    "acc_40840Q16 = accuracy_score(s_true40840Q16, s_pred40840Q16_stm, normalize=True)\n",
    "acc_40840orig = accuracy_score(s_true40840orig, s_pred40840orig_stm, normalize=True)\n",
    "\n",
    "print(\"2530 Orig ACC: \", \"{:.2f}\".format(acc_2530orig))\n",
    "print(\"2530 Q32 ACC: \", \"{:.2f}\".format(acc_2530Q32))\n",
    "print(\"2530 Q16 ACC: \", \"{:.2f}\".format(acc_2530Q16))\n",
    "print()\n",
    "print(\"14241 Orig ACC: \", \"{:.2f}\".format(acc_14241orig))\n",
    "print(\"14241 Q32 ACC: \", \"{:.2f}\".format(acc_14241Q32))\n",
    "print(\"14241 Q16 ACC: \", \"{:.2f}\".format(acc_14241Q16))\n",
    "print()\n",
    "print(\"23625 Orig ACC: \", \"{:.2f}\".format(acc_23625orig))\n",
    "print(\"23625 Q32 ACC: \", \"{:.2f}\".format(acc_23625Q32))\n",
    "print(\"23625 Q16 ACC: \", \"{:.2f}\".format(acc_23625Q16))\n",
    "print()\n",
    "print(\"24160 Orig ACC: \", \"{:.2f}\".format(acc_24160orig))\n",
    "print(\"24160 Q32 ACC: \", \"{:.2f}\".format(acc_24160Q32))\n",
    "print(\"24160 Q16 ACC: \", \"{:.2f}\".format(acc_24160Q16))\n",
    "print()\n",
    "print(\"40840 Orig ACC: \", \"{:.2f}\".format(acc_40840orig))\n",
    "print(\"40840 Q32 ACC: \", \"{:.2f}\".format(acc_40840Q32))\n",
    "print(\"40840 Q16 ACC: \", \"{:.2f}\".format(acc_40840Q16))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
